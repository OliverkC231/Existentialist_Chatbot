# import basics
import os
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document

load_dotenv(".env.ollama")
# initialize pinecone database
pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))

# set the pinecone index
index_name = os.environ.get("PINECONE_INDEX_NAME")
index = pc.Index(index_name)

# initialize mxbai embeddings (Ollama)
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# initialize the vector store
vector_store = PineconeVectorStore(index=index, embedding=embeddings)

# retrieval
retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"k": 5, "score_threshold": 0.5},
)

results = retriever.invoke("what is retrieval augmented generation?")

# show results
print("RESULTS:")
for res in results:
    print(f"* {res.page_content} [{res.metadata}]")
